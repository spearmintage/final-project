{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModel(nn.Module):\n",
    "    global folder_df\n",
    "\n",
    "    def __init__(self, input_shape: torch.Size, dropout_rate: float = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        total_output_classes = len(folder_df[\"folder\"].unique())\n",
    "\n",
    "        # input shape should be some list/tuple of length 4\n",
    "        if len(input_shape) != 4: return Exception(\"Input shape is not AxBxCxD.\")\n",
    "        \n",
    "        A = input_shape[0]\n",
    "        B = input_shape[1]\n",
    "        C = input_shape[2]\n",
    "        D = input_shape[3]\n",
    "\n",
    "        self.relu = nn.ReLU() # relu does not have trainable parameters, thus, can be reused\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=B, out_channels=10, kernel_size=(3, 3))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.drop1 = nn.Dropout(p=dropout_rate)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.drop2 = nn.Dropout(p=dropout_rate)\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        conv_layers = 3\n",
    "        pool_layers = 3\n",
    "        final_width = C\n",
    "        final_height = D\n",
    "\n",
    "        # ASSUMES KERNEL SIZE IS 3 AND 2 FOR CONV AND POOL LAYERS\n",
    "        while conv_layers > 0 and pool_layers > 0:\n",
    "            if conv_layers > 0:\n",
    "                final_width = final_width - 2\n",
    "                final_height = final_height - 2\n",
    "                conv_layers -= 1\n",
    "            if pool_layers > 0:\n",
    "                final_width = final_width // 2\n",
    "                final_height = final_height // 2\n",
    "                pool_layers -= 1\n",
    "\n",
    "        flatten_nodes = 10 * final_width * final_height\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d(num_features=flatten_nodes)\n",
    "        self.linear1 = nn.Linear(in_features=flatten_nodes, out_features=1024)\n",
    "        self.linear2 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.linear3 = nn.Linear(in_features=512, out_features=128) \n",
    "        self.output = nn.Linear(in_features=128, out_features=total_output_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define calculations here\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.pool2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = self.flat(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.output(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
