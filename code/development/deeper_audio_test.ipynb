{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal:\n",
    "\n",
    "- use librosa on 1000 audio files and/or 50 class folders minimum\n",
    "- store information in a dataframe (excluding actual file data for now)\n",
    "  - folder name, file name, sampling rate, data length, file length, file size(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T21:32:20.381671Z",
     "iopub.status.busy": "2026-02-04T21:32:20.381522Z",
     "iopub.status.idle": "2026-02-04T21:32:26.557313Z",
     "shell.execute_reply": "2026-02-04T21:32:26.556909Z",
     "shell.execute_reply.started": "2026-02-04T21:32:20.381652Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import copy\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Audio\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from math import floor\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T21:32:26.558053Z",
     "iopub.status.busy": "2026-02-04T21:32:26.557833Z",
     "iopub.status.idle": "2026-02-04T21:32:26.560730Z",
     "shell.execute_reply": "2026-02-04T21:32:26.560339Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.558038Z"
    }
   },
   "outputs": [],
   "source": [
    "# experimental parameters for model training, for simple configuration in a single cell.\n",
    "# very useful for reporting spreadsheet\n",
    "\n",
    "on_sciama = True # for changing source directory\n",
    "\n",
    "folder_key = {}\n",
    "\n",
    "split_interval_secs = 2\n",
    "split_length = 10000000000000 # absurd number for future use\n",
    "sample_rate = 32000\n",
    "\n",
    "variance_threshold = 0.2\n",
    "mean_threshold = 5\n",
    "\n",
    "total_folders = 50\n",
    "min_rating = 5.0\n",
    "max_files_per_folder = 0\n",
    "\n",
    "sampling_technique = \"median\"\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 2048\n",
    "dropout_rate = 0.5\n",
    "learning_rate = 0.001\n",
    "momentum = 0\n",
    "experimental = False\n",
    "experimental_counter_limit = 10\n",
    "save_best_to_file = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T21:32:26.561176Z",
     "iopub.status.busy": "2026-02-04T21:32:26.561051Z",
     "iopub.status.idle": "2026-02-04T21:32:26.564827Z",
     "shell.execute_reply": "2026-02-04T21:32:26.564482Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.561164Z"
    }
   },
   "outputs": [],
   "source": [
    "if on_sciama:\n",
    "    base_path = \"/mnt/lustre/peprmint/train_audio/\"\n",
    "else:\n",
    "    base_path = \"../../datasets/birdsongs-combined/train_audio/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T21:32:26.565273Z",
     "iopub.status.busy": "2026-02-04T21:32:26.565152Z",
     "iopub.status.idle": "2026-02-04T21:32:26.573790Z",
     "shell.execute_reply": "2026-02-04T21:32:26.573508Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.565261Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def reset_seed():\n",
    "    torch.manual_seed(1368)\n",
    "    random.seed(1368)\n",
    "    np.random.seed(1368)\n",
    "\n",
    "reset_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T21:32:26.574189Z",
     "iopub.status.busy": "2026-02-04T21:32:26.574074Z",
     "iopub.status.idle": "2026-02-04T21:32:26.718426Z",
     "shell.execute_reply": "2026-02-04T21:32:26.718123Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.574177Z"
    },
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85094825984\n",
      "cuda\n",
      "NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i in range(torch.cuda.device_count()):\n",
    "   print(torch.cuda.get_device_properties(i).total_memory)\n",
    "\n",
    "print(device)\n",
    "print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-04T21:32:26.719651Z",
     "iopub.status.busy": "2026-02-04T21:32:26.719520Z",
     "iopub.status.idle": "2026-02-04T21:32:26.881764Z",
     "shell.execute_reply": "2026-02-04T21:32:26.880969Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.719639Z"
    },
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_random_folders_filtered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m reset_seed()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m folder_df = \u001b[43mget_random_folders_filtered\u001b[49m(\n\u001b[32m      4\u001b[39m     base_path, \n\u001b[32m      5\u001b[39m     split_interval_secs=split_interval_secs, \n\u001b[32m      6\u001b[39m     sample_rate=sample_rate,\n\u001b[32m      7\u001b[39m     total_folders=total_folders,\n\u001b[32m      8\u001b[39m     min_rating=min_rating,\n\u001b[32m      9\u001b[39m     max_files_per_folder=max_files_per_folder\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'get_random_folders_filtered' is not defined"
     ]
    }
   ],
   "source": [
    "reset_seed()\n",
    "\n",
    "folder_df = get_random_folders_filtered(\n",
    "    base_path, \n",
    "    split_interval_secs=split_interval_secs, \n",
    "    sample_rate=sample_rate,\n",
    "    total_folders=total_folders,\n",
    "    min_rating=min_rating,\n",
    "    max_files_per_folder=max_files_per_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.882060Z",
     "iopub.status.idle": "2026-02-04T21:32:26.882215Z",
     "shell.execute_reply": "2026-02-04T21:32:26.882144Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.882136Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# base_dir: directory of all the audio file folders\n",
    "# split_interval_secs: the length of each sound clip when split up in a file.\n",
    "#   - final dataframe will not contain any sound clips over/under this length\n",
    "# sample_rate: sample rate of sound clips in hz\n",
    "# total_folders: total folders to iterate through the files, given range of files per folder\n",
    "# min_files_per_folder: minimum files to exist in the folder\n",
    "# max_files_per_folder: maximum files to use inside the folder\n",
    "def get_random_folders_filtered(base_dir: str, split_interval_secs: float, sample_rate: int = 32000, total_folders: int = 50, max_files_per_folder: int = -1, min_rating: float = 4.0):\n",
    "    global split_length, folder_key\n",
    "\n",
    "    # get the list of valid folders and files to analyse\n",
    "    folder_key = {}\n",
    "\n",
    "    folders = []\n",
    "    valid_folders = set()\n",
    "    valid_files = set([l.replace(\"\\n\", \"\") for l in open(f\"rating_thresholds_at_least_10_classes/min_rating_{int(min_rating * 2)}.txt\").readlines()])\n",
    "    for f in valid_files: valid_folders.add(f.split(\"/\")[0])\n",
    "    for folder in [line.replace(\"\\n\", \"\") for line in open(\"folder_order.txt\").readlines()]:\n",
    "        if folder in valid_folders:\n",
    "            folders.append(folder)\n",
    "\n",
    "    folder_count = 0\n",
    "\n",
    "    # if there are less folders than those specified, view all folders instead of given amount\n",
    "    total_folders = min(len(folders), total_folders)\n",
    "\n",
    "    rows = []\n",
    "    \n",
    "    for folder in folders:\n",
    "        \n",
    "        folder_path = base_dir + folder + \"/\"\n",
    "        valid_folder_files = []\n",
    "        # only iterate through the valid audio files\n",
    "        for file in os.listdir(folder_path):\n",
    "            if folder + \"/\" + file in valid_files:\n",
    "                valid_folder_files.append(folder_path + file)\n",
    "        random.shuffle(valid_folder_files)\n",
    "\n",
    "        if max_files_per_folder > 0:\n",
    "            valid_folder_files = valid_folder_files[:min(max_files_per_folder, len(valid_folder_files))]\n",
    "\n",
    "        # iterate through each file in the folder\n",
    "        for file_path in (progress_bar := tqdm(valid_folder_files)):\n",
    "            # load file data and resample to sample_rate if necessary\n",
    "            file_data, file_sample_rate_hz = torchaudio.load(uri=file_path, channels_first=True)\n",
    "            if file_sample_rate_hz != sample_rate:\n",
    "                file_data = torchaudio.functional.resample(file_data, orig_freq=file_sample_rate_hz, new_freq=sample_rate)\n",
    "\n",
    "            # convert all audio into mono (1 channel) if audio is stereo (2 channels)\n",
    "            if file_data.shape[0] == 2:\n",
    "                file_data = file_data.mean(dim=0)\n",
    "            else:\n",
    "                file_data = file_data.flatten()\n",
    "\n",
    "            # get total number of X second splits\n",
    "            total_splits = floor(len(file_data) / int(sample_rate * split_interval_secs))\n",
    "            \n",
    "            # convert file data into mel-spectrogram fourier transform for feeding into CNN\n",
    "            n_fft = 1024\n",
    "\n",
    "\n",
    "\n",
    "            mel_spec_transform = torchaudio.transforms.MelSpectrogram(sample_rate = sample_rate, power=2, n_fft=n_fft)\n",
    "            \n",
    "            # OLD MEL-SPECTROGRAM TRANSFORMATION\n",
    "            # amp_to_db_transform = torchaudio.transforms.AmplitudeToDB(stype=\"amplitude\", top_db=80)\n",
    "            # mel_spec_data_db = amp_to_db_transform(mel_spec_transform(file_data)).numpy()\n",
    "            \n",
    "            # NEW MEL-SPECTROGRAM TRANSFORMATION\n",
    "            mel_spec_data_db = torchaudio.functional.amplitude_to_DB(mel_spec_transform(file_data), multiplier=20., amin=0, db_multiplier=1, top_db=80).numpy()\n",
    "            mel_spec_data_db = mel_spec_data_db - mel_spec_data_db.min()\n",
    "            \n",
    "\n",
    "            \n",
    "            # if file is at least X seconds.\n",
    "            if total_splits >= 1:\n",
    "                split_length = min(mel_spec_data_db.shape[1] // total_splits, split_length)\n",
    "                mel_spec_splits = np.arange(0, mel_spec_data_db.shape[1], split_length)\n",
    "                for i in range(len(mel_spec_splits) - 1):\n",
    "                    start = mel_spec_splits[i]\n",
    "                    end = mel_spec_splits[i + 1]\n",
    "\n",
    "                    mel_spec_split = mel_spec_data_db[:, start:end]\n",
    "\n",
    "                    row = {}\n",
    "                    row[\"folder\"] = folder\n",
    "                    row[\"file\"] = file_path.split(\"/\")[-1]\n",
    "                    row[\"mel_spec\"] = mel_spec_split\n",
    "                    row[\"mel_spec_shape\"] = mel_spec_split.shape\n",
    "\n",
    "                    # determine and filter for \"silence\":\n",
    "                    var = np.var(mel_spec_split)\n",
    "                    mean = np.mean(mel_spec_split)\n",
    "\n",
    "                    # for identifying and removing/changing silent noise\n",
    "                    if mean <= mean_threshold and var <= variance_threshold:\n",
    "                        row[\"folder\"] = \"SILENT\"\n",
    "                        \n",
    "                    rows.append(row)\n",
    "\n",
    "                folder_key[folder_count] = folder\n",
    "            \n",
    "            progress_bar.set_description(f\"Folder {folder_count + 1}/{total_folders} - {folder}\")\n",
    "        \n",
    "        # end loop if total_folders has been reached\n",
    "        folder_count += 1\n",
    "        if folder_count >= total_folders: break\n",
    "\n",
    "    # trim the ends of some mel spectrograms because of stupid floating point nonsense\n",
    "    for i, row in enumerate(rows):\n",
    "        if row[\"mel_spec_shape\"][1] != split_length:\n",
    "            row[\"mel_spec\"] = row[\"mel_spec\"][:, :split_length]\n",
    "            row[\"mel_spec_shape\"] = row[\"mel_spec\"].shape\n",
    "\n",
    "    folder_key[total_folders] = \"SILENT\"\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.882425Z",
     "iopub.status.idle": "2026-02-04T21:32:26.882582Z",
     "shell.execute_reply": "2026-02-04T21:32:26.882510Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.882502Z"
    }
   },
   "outputs": [],
   "source": [
    "folder_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.882869Z",
     "iopub.status.idle": "2026-02-04T21:32:26.883012Z",
     "shell.execute_reply": "2026-02-04T21:32:26.882946Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.882939Z"
    }
   },
   "outputs": [],
   "source": [
    "folder_df[\"mel_spec_shape\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.883241Z",
     "iopub.status.idle": "2026-02-04T21:32:26.883380Z",
     "shell.execute_reply": "2026-02-04T21:32:26.883314Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.883307Z"
    }
   },
   "outputs": [],
   "source": [
    "folder_df[\"folder\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.883606Z",
     "iopub.status.idle": "2026-02-04T21:32:26.883745Z",
     "shell.execute_reply": "2026-02-04T21:32:26.883681Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.883674Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"sound clips BEFORE any sampling:\", len(folder_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.883975Z",
     "iopub.status.idle": "2026-02-04T21:32:26.884109Z",
     "shell.execute_reply": "2026-02-04T21:32:26.884046Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.884039Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "len(folder_df[\"folder\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.884340Z",
     "iopub.status.idle": "2026-02-04T21:32:26.884476Z",
     "shell.execute_reply": "2026-02-04T21:32:26.884411Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.884405Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(max(10, total_folders // 8), 6))\n",
    "folder_df[\"folder\"].value_counts().plot.bar(title=\"Value counts before over/under sampling.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.884706Z",
     "iopub.status.idle": "2026-02-04T21:32:26.884846Z",
     "shell.execute_reply": "2026-02-04T21:32:26.884779Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.884772Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply random undersampling OR oversampling\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "if sampling_technique == \"under\":\n",
    "    # random \"naive\" undersampling\n",
    "    # delete random rows from all classes except minority class(es)\n",
    "    min_folder_count = min(folder_df[\"folder\"].value_counts())\n",
    "\n",
    "    indices_to_keep = np.array([])\n",
    "\n",
    "    for f in folder_df[\"folder\"].unique():\n",
    "        indices_to_keep = np.concat([np.random.choice(folder_df[folder_df[\"folder\"] == f].index, size=min_folder_count), indices_to_keep])\n",
    "\n",
    "    folder_df = folder_df.loc[indices_to_keep]\n",
    "    folder_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "if sampling_technique == \"over\":\n",
    "    ros = RandomOverSampler(random_state=1368)\n",
    "\n",
    "    # print how much each class will increase by (relative to its current size)\n",
    "    max_folder = max(folder_df[\"folder\"].value_counts())\n",
    "    for folder, count in folder_df[\"folder\"].value_counts().sort_values(ascending=True).items():\n",
    "        print(f\"{folder} will increase by {round((max_folder / count - 1) * 100, 2)}%\")\n",
    "\n",
    "    folder_df = pd.concat(ros.fit_resample(folder_df.drop(\"folder\", axis=1), folder_df[\"folder\"]), axis=1)\n",
    "    # moves \"folder\" to other side of df but shouldn't affect anything\n",
    "\n",
    "if sampling_technique == \"median\":\n",
    "    # take the median value of folder counts\n",
    "    # under sample all above the median\n",
    "    # over smaple all below the median\n",
    "\n",
    "    ros = RandomOverSampler(random_state=1368)\n",
    "    median_folder_count = round(folder_df[\"folder\"].value_counts().median())\n",
    "    indices_to_keep = np.array([])\n",
    "\n",
    "    print(\"median:\", median_folder_count)\n",
    "\n",
    "    for f in folder_df[\"folder\"].unique():\n",
    "        folder_length = len(folder_df[folder_df[\"folder\"] == f])\n",
    "        if folder_length > median_folder_count:\n",
    "            indices_to_keep = np.concat([np.random.choice(folder_df[folder_df[\"folder\"] == f].index, size=median_folder_count), indices_to_keep])\n",
    "        else:\n",
    "            indices_to_keep = np.concat([folder_df[folder_df[\"folder\"] == f].index, indices_to_keep])\n",
    "        \n",
    "    folder_df = folder_df.loc[indices_to_keep]\n",
    "    folder_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    folder_df = pd.concat(ros.fit_resample(folder_df.drop(\"folder\", axis=1), folder_df[\"folder\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.885173Z",
     "iopub.status.idle": "2026-02-04T21:32:26.885307Z",
     "shell.execute_reply": "2026-02-04T21:32:26.885244Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.885237Z"
    }
   },
   "outputs": [],
   "source": [
    "folder_df[\"folder\"].value_counts().plot.bar(title=f\"Value counts after {sampling_technique} sampling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.885521Z",
     "iopub.status.idle": "2026-02-04T21:32:26.885655Z",
     "shell.execute_reply": "2026-02-04T21:32:26.885592Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.885585Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"sound clips AFTER sampling:\", len(folder_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.885904Z",
     "iopub.status.idle": "2026-02-04T21:32:26.886041Z",
     "shell.execute_reply": "2026-02-04T21:32:26.885975Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.885968Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# plot 5 random mel spectrograms\n",
    "# checks for consistency in scaling and whatnot\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "sample_spec_indices = random.sample(folder_df.index.tolist(), k=3)\n",
    "\n",
    "for i in sample_spec_indices:\n",
    "    row = folder_df.iloc[i]\n",
    "    \n",
    "    print(row[\"mel_spec_shape\"])\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(row[\"mel_spec\"])\n",
    "    plt.title(f\"Mel-Spectrogram of {'/'.join(row['file'].split('/')[-2:])}\")\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.show()\n",
    "\n",
    "del row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.886269Z",
     "iopub.status.idle": "2026-02-04T21:32:26.886403Z",
     "shell.execute_reply": "2026-02-04T21:32:26.886341Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.886334Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# input shape needs to be AxBxCxD\n",
    "# A = list of files\n",
    "# B = depth of each one (currently just 1, as it only contains 1 value. if colours are used then maybe up it to 3)\n",
    "# CxD = input width/height (128x626 etc)\n",
    "if \"mel_spec\" in folder_df.columns:\n",
    "    print(\"step 1\")\n",
    "    x = folder_df[\"mel_spec\"]\n",
    "    print(\"step 2\")\n",
    "    folder_df.drop(\"mel_spec\", axis=1, inplace=True)\n",
    "    print(\"step 3\")\n",
    "    x = np.stack(x)\n",
    "    print(\"step 4\")\n",
    "    x = torch.from_numpy(x)\n",
    "    print(\"step 5\")\n",
    "    x = x.reshape(x.shape[0], 1, x.shape[1], x.shape[2])\n",
    "    print(\"step 6\")\n",
    "    x = x.to(device)\n",
    "\n",
    "print(\"step 7\")\n",
    "le = LabelEncoder()\n",
    "print(\"step 8\")\n",
    "y = torch.LongTensor(le.fit_transform(folder_df[\"folder\"]))\n",
    "print(\"step 9\")\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.886653Z",
     "iopub.status.idle": "2026-02-04T21:32:26.886787Z",
     "shell.execute_reply": "2026-02-04T21:32:26.886725Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.886718Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "reset_seed()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, shuffle=True, test_size=test_size, random_state=1368)\n",
    "\n",
    "del x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.887001Z",
     "iopub.status.idle": "2026-02-04T21:32:26.887132Z",
     "shell.execute_reply": "2026-02-04T21:32:26.887071Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.887065Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# iteration 3.0\n",
    "\n",
    "class TestModel(nn.Module):\n",
    "    global folder_df\n",
    "\n",
    "    def __init__(self, input_shape: torch.Size, dropout_rate: float = 0):\n",
    "        super().__init__()\n",
    "\n",
    "        total_output_classes = len(folder_df[\"folder\"].unique())\n",
    "\n",
    "        # input shape should be some list/tuple of length 4\n",
    "        if len(input_shape) != 4: return Exception(\"Input shape is not AxBxCxD.\")\n",
    "        \n",
    "        A = input_shape[0]\n",
    "        B = input_shape[1]\n",
    "        C = input_shape[2]\n",
    "        D = input_shape[3]\n",
    "\n",
    "        self.relu = nn.ReLU6() # relu does not have trainable parameters, thus, can be reused\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=B, out_channels=10, kernel_size=(3, 3))\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.drop1 = nn.Dropout(p=dropout_rate)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=10, kernel_size=(3, 3))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.drop2 = nn.Dropout(p=dropout_rate)\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        flatten_nodes = 10 * ((((C - 2) // 2) - 2) // 2) * ((((D - 2) // 2) - 2) // 2)\n",
    "        \n",
    "        self.norm = nn.BatchNorm1d(num_features=flatten_nodes)\n",
    "        self.linear1 = nn.Linear(in_features=flatten_nodes, out_features=1024)\n",
    "        self.linear2 = nn.Linear(in_features=1024, out_features=512)\n",
    "        self.linear3 = nn.Linear(in_features=512, out_features=128) \n",
    "        self.output = nn.Linear(in_features=128, out_features=total_output_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # define calculations here\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.pool2(x)\n",
    "        x = self.drop2(x)\n",
    "\n",
    "        x = self.flat(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.output(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.887418Z",
     "iopub.status.idle": "2026-02-04T21:32:26.887562Z",
     "shell.execute_reply": "2026-02-04T21:32:26.887489Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.887483Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# model: the model to train\n",
    "# loss_fn: the loss function to update weights\n",
    "# optimizer: the optimiser function to affect rate of change\n",
    "# epochs: number of epochs to train the model over\n",
    "# batch_size: samples to train x_train and y_train to avoid memory issues\n",
    "# save_best_to_file: save model with highest test accuracy to models/ folder.\n",
    "# experimental: whether to end training after 5 consecutive runs of no higher test accuracy.\n",
    "#       - training ends when either [epochs] epochs have been run, *OR* if 5 consecutive runs do not improve test accuracy from previous best.\n",
    "\n",
    "def train_model(model, loss_fn, optimizer, epochs, batch_size, save_best_to_file = False, experimental = False):\n",
    "    global x_train, y_train\n",
    "    batch_indices = np.linspace(0, len(x_train), int(len(x_train) / batch_size), dtype=\"int\")\n",
    "\n",
    "    train_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    test_acc_hist = []\n",
    "\n",
    "    best_test_acc = 0\n",
    "    best_model = None\n",
    "\n",
    "    experimental_counter = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        train_acc = 0\n",
    "        \n",
    "        for i in range(len(batch_indices) - 1):\n",
    "            start_index = batch_indices[i]\n",
    "            stop_index = batch_indices[i + 1]\n",
    "\n",
    "            x_batch = x_train[start_index:stop_index]\n",
    "            y_batch = y_train[start_index:stop_index]\n",
    "\n",
    "            x_batch, y_batch = shuffle(x_batch, y_batch)\n",
    "\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            train_acc += sum(torch.argmax(y_pred, dim=1) == y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_acc = train_acc * 100 / len(y_train)\n",
    "\n",
    "        model.eval()\n",
    "        test_acc = sum(torch.argmax(model(x_test), dim=1) == y_test) * 100 / len(y_test)\n",
    "\n",
    "        print(f\"Epoch {epoch}: train loss = {loss:.04f}, train acc = {train_acc:.02f}%, test acc = {test_acc:.02f}% {\"!!\" if test_acc > best_test_acc else \"\"}\")\n",
    "\n",
    "        train_loss_hist.append(float(loss.cpu().detach().numpy()))\n",
    "        train_acc_hist.append(float(train_acc.cpu().detach().numpy()))\n",
    "        test_acc_hist.append(float(test_acc.cpu().detach().numpy()))\n",
    "\n",
    "        if experimental:\n",
    "            experimental_counter += 1\n",
    "\n",
    "        if test_acc > best_test_acc:\n",
    "            experimental_counter = 0\n",
    "            best_test_acc = test_acc\n",
    "            \n",
    "            # overwrite best model with new best model\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if experimental_counter == experimental_counter_limit:\n",
    "            print(f\"Experimental Mode ended the training early, as the testing accuracy had not increased over {experimental_counter_limit} consecutive epochs.\")\n",
    "            break\n",
    "        \n",
    "    if save_best_to_file:\n",
    "        model_save_path = \"models/\"\n",
    "        if not os.path.exists(model_save_path): os.mkdir(model_save_path)\n",
    "        file_number = len(os.listdir(model_save_path)) + 1\n",
    "        file_name = f\"best_model_{file_number:>03}\"\n",
    "        best_model_path = model_save_path + file_name + \".pth\"\n",
    "\n",
    "        torch.save(best_model, best_model_path)\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    pd.Series(train_acc_hist).plot.line(label=\"Train\", color=\"blue\")\n",
    "    pd.Series(test_acc_hist).plot.line(label=\"Test\", color=\"red\")\n",
    "    plt.legend()\n",
    "    plt.ylim((0, 100))\n",
    "    plt.yticks(np.arange(0, 101, 10))\n",
    "    plt.title(f\"Convolutional Neural Network on {len(folder_df)} sound clips over {len(folder_df[\"folder\"].unique())} folders.\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.grid(axis=\"y\")\n",
    "    plt.show()\n",
    "\n",
    "    return train_loss_hist, train_acc_hist, test_acc_hist, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.887788Z",
     "iopub.status.idle": "2026-02-04T21:32:26.887922Z",
     "shell.execute_reply": "2026-02-04T21:32:26.887858Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.887852Z"
    },
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# to avoid extreme overfitting:\n",
    "# - dropout rate between 0.2-0.5 seems good\n",
    "# - learning rate around 0.001 provides best training rate without overfitting\n",
    "# - train in larger batches, 64 upwards seems good\n",
    "# - most seem to plateau around 200 epochs, maybe reduce to reduce total training time\n",
    "\n",
    "reset_seed()\n",
    "\n",
    "model = TestModel(input_shape=x_train.shape, dropout_rate=dropout_rate)\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss() # since we are classifying.\n",
    "loss_fn = loss_fn.to(device)\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate) # adam has no momentum, but DOES have weight decay\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "train_loss_hist, train_acc_hist, test_acc_hist, best_model = train_model(model, loss_fn, optimizer, epochs, batch_size, save_best_to_file=save_best_to_file, experimental=experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.888232Z",
     "iopub.status.idle": "2026-02-04T21:32:26.888368Z",
     "shell.execute_reply": "2026-02-04T21:32:26.888305Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.888298Z"
    }
   },
   "outputs": [],
   "source": [
    "# investigate TOP 5 ERROR of the model.\n",
    "# compare with top 1 error (accuracy)\n",
    "\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "# get predictions for ALL testing data\n",
    "k = 5\n",
    "test_pred = model(x_test)\n",
    "test_pred_top_k = torch.topk(test_pred, k=k, dim=1)[1]\n",
    "\n",
    "total = 0\n",
    "in_top_k = 0\n",
    "for index, value in enumerate(y_test):\n",
    "    if value in test_pred_top_k[index]: in_top_k += 1\n",
    "\n",
    "    total += 1\n",
    "\n",
    "top_k_accuracy = in_top_k * 100 / total\n",
    "\n",
    "print(f\"Top {k} Accuracy: {top_k_accuracy:.03f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.888605Z",
     "iopub.status.idle": "2026-02-04T21:32:26.888737Z",
     "shell.execute_reply": "2026-02-04T21:32:26.888676Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.888669Z"
    }
   },
   "outputs": [],
   "source": [
    "folder_df[\"file\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.888968Z",
     "iopub.status.idle": "2026-02-04T21:32:26.889100Z",
     "shell.execute_reply": "2026-02-04T21:32:26.889038Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.889031Z"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_df = pd.DataFrame(data=[y_test.cpu().numpy(), test_pred_top_k.cpu().numpy()], index=[\"actual\", \"pred\"]).transpose()\n",
    "accuracy_df[\"in_top_5\"] = [1 if accuracy_df.iloc[i, 0] in accuracy_df.iloc[i, 1] else 0 for i in range(len(accuracy_df.index))]\n",
    "folder_top_5 = accuracy_df.groupby(by=\"actual\").aggregate(func=np.sum)[\"in_top_5\"].values\n",
    "folder_counts = accuracy_df.groupby(by=\"actual\").aggregate(func=np.size)[\"in_top_5\"].values\n",
    "accuracy_df = pd.DataFrame(data=[folder_top_5, folder_counts], index=[\"in_top_5\", \"count\"]).transpose()\n",
    "accuracy_df[\"top_5_accuracy\"] = round(accuracy_df[\"in_top_5\"] * 100 / accuracy_df[\"count\"], 2)\n",
    "accuracy_df.sort_values(by=\"top_5_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.889359Z",
     "iopub.status.idle": "2026-02-04T21:32:26.889501Z",
     "shell.execute_reply": "2026-02-04T21:32:26.889429Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.889423Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes in a whole file as input, feeds into the model, performs sequential aggregation\n",
    "# gives a single output based on aggregation strategy\n",
    "def test_file(folder, file_path):\n",
    "    global sample_rate, split_length, model\n",
    "    samples = []\n",
    "    \n",
    "    # load file data and resample to sample_rate if necessary\n",
    "    file_data, file_sample_rate_hz = torchaudio.load(uri=file_path, channels_first=True)\n",
    "    if file_sample_rate_hz != sample_rate:\n",
    "        file_data = torchaudio.functional.resample(file_data, orig_freq=file_sample_rate_hz, new_freq=sample_rate)\n",
    "\n",
    "    # convert all audio into mono (1 channel) if audio is stereo (2 channels)\n",
    "    if file_data.shape[0] == 2: file_data = file_data.mean(dim=0)\n",
    "    else: file_data = file_data.flatten()\n",
    "\n",
    "    # get total number of X second splits\n",
    "    total_splits = floor(len(file_data) / int(sample_rate * split_interval_secs))\n",
    "    \n",
    "    # convert file data into mel-spectrogram fourier transform for feeding into CNN\n",
    "    n_fft = 1024\n",
    "\n",
    "    mel_spec_transform = torchaudio.transforms.MelSpectrogram(sample_rate = sample_rate, power=2, n_fft=n_fft)\n",
    "    amp_to_db_transform = torchaudio.transforms.AmplitudeToDB(stype=\"amplitude\", top_db=80)\n",
    "    mel_spec_data_db = amp_to_db_transform(mel_spec_transform(file_data))\n",
    "    \n",
    "    # if file is at least X seconds.\n",
    "    if total_splits >= 1:\n",
    "        split_length = min(mel_spec_data_db.shape[1] // total_splits, split_length)\n",
    "        mel_spec_splits = np.arange(0, mel_spec_data_db.shape[1], split_length)\n",
    "        for i in range(len(mel_spec_splits) - 1):\n",
    "            start = mel_spec_splits[i]\n",
    "            end = mel_spec_splits[i + 1]\n",
    "\n",
    "            mel_spec_split = mel_spec_data_db[:, start:end]\n",
    "\n",
    "            var = np.var(mel_spec_split.numpy())\n",
    "            mean = np.mean(mel_spec_split.numpy())\n",
    "\n",
    "            #if var > variance_threshold:\n",
    "            # automatically keep anything ABOVE mean db level\n",
    "            if mean > mean_threshold:\n",
    "                samples.append(mel_spec_split.reshape(1, 128, -1).numpy())\n",
    "            else:\n",
    "                # if mean is LOW, only include those with variance ABOVE threshold\n",
    "                if var > variance_threshold:\n",
    "                    samples.append(mel_spec_split.reshape(1, 128, -1).numpy())\n",
    "            \n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    samples = torch.Tensor(np.array(samples)).to(device)\n",
    "\n",
    "    preds = model(samples)\n",
    "\n",
    "\n",
    "    '''\n",
    "    # sequential aggregation via max value output per index/folder\n",
    "    seq_aggregation_results = {}\n",
    "    m = torch.max(preds, dim=1)\n",
    "    for index, folder in folder_key.items():\n",
    "        seq_aggregation_results[folder] = max(seq_aggregation_results.get(folder, 0), 0)\n",
    "\n",
    "    for index in range(len(m.indices)):\n",
    "        folder_index = m.indices[index].item()\n",
    "        value = m.values[index].item()\n",
    "    \n",
    "        seq_aggregation_results[folder_key[folder_index]] = max(seq_aggregation_results.get(folder_key[folder_index], 0), value)\n",
    "    '''\n",
    "\n",
    "    \n",
    "    # Ax[classes] sized array, A = number of splits in file\n",
    "    # sequential aggregation via summation (appears to be the best solution right now, but barely by much)\n",
    "    seq_aggregation_result = torch.sum(preds, dim=0)\n",
    "    prediction_order = [folder_key[i] for i in torch.topk(seq_aggregation_result, k=len(seq_aggregation_result)).indices.cpu().numpy() if folder_key[i] != \"SILENT\"]\n",
    "    location = prediction_order.index(folder) + 1 # because of zero index, adjust to make it 1-indexed\n",
    "    \n",
    "    return location, prediction_order\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # sequential aggregation via ranking\n",
    "    # - assign each folder a rank (using topk) for each prediction, starting from 0 or 1.\n",
    "    # - sum the ranks over each folder\n",
    "    # - arrange each rank sum from smallest to largest\n",
    "    # - smaller sums = higher predictions\n",
    "    folder_ranks = {}\n",
    "    \n",
    "    for pred in preds:\n",
    "        ranks = torch.topk(pred, k=5).indices.cpu().numpy()\n",
    "\n",
    "        rank_value = 1 # can start at 0 as well. #1 makes sense for \"first place\"\n",
    "        for rank in ranks:\n",
    "            key = folder_key[rank]\n",
    "            folder_ranks[key] = folder_ranks.get(key, 0) + rank_value\n",
    "\n",
    "            rank_value += 1\n",
    "\n",
    "    folder_ranks = dict(sorted(folder_ranks.items(), key=lambda x: x[1]))\n",
    "    \n",
    "    location = list(folder_ranks.keys()).index(folder) + 1 if folder in list(folder_ranks.keys()) else 50000\n",
    "\n",
    "    return location, list(folder_ranks.keys())\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.889710Z",
     "iopub.status.idle": "2026-02-04T21:32:26.889843Z",
     "shell.execute_reply": "2026-02-04T21:32:26.889780Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.889773Z"
    }
   },
   "outputs": [],
   "source": [
    "is_valid_file = False\n",
    "\n",
    "valid_files = valid_files = set([l.replace(\"\\n\", \"\") for l in open(f\"rating_thresholds_at_least_10_classes/min_rating_{int(min_rating * 2)}.txt\").readlines()])\n",
    "\n",
    "while not is_valid_file:\n",
    "    random_folder = base_path + random.choice([i for i in folder_key.values() if i != \"SILENT\"]) + \"/\"\n",
    "    random_file = random_folder + random.choice(os.listdir(random_folder))\n",
    "\n",
    "    if random_file.split(\"/\")[-2] + \"/\" + random_file.split(\"/\")[-1] in valid_files:\n",
    "        is_valid_file = True\n",
    "\n",
    "print(random_file)\n",
    "\n",
    "test_file(random_folder.split(\"/\")[-2], random_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.890086Z",
     "iopub.status.idle": "2026-02-04T21:32:26.890219Z",
     "shell.execute_reply": "2026-02-04T21:32:26.890157Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.890150Z"
    }
   },
   "outputs": [],
   "source": [
    "# do above but for EVERY SINGLE FILE in the classes used\n",
    "\n",
    "valid_files = set([l.replace(\"\\n\", \"\") for l in open(f\"rating_thresholds_at_least_10_classes/min_rating_{int(min_rating * 2)}.txt\").readlines()])\n",
    "sequential_aggregation_results = {}\n",
    "\n",
    "folder_counter = 1\n",
    "for folder in folder_key.values():\n",
    "\n",
    "    if folder == \"SILENT\":\n",
    "        folder_counter += 1\n",
    "        continue\n",
    "\n",
    "    folder_path = base_path + folder + \"/\"\n",
    "\n",
    "    folder_length = len(os.listdir(folder_path))\n",
    "    file_counter = 0\n",
    "    for file in (progress_bar := tqdm(os.listdir(folder_path))):\n",
    "        file_counter += 1\n",
    "        progress_bar.set_description(f\"{folder_counter}/{len(folder_key.keys())} - {folder} - {file_counter}/{folder_length}\")\n",
    "        \n",
    "        file_path = folder_path + file\n",
    "\n",
    "        if folder + \"/\" + file in valid_files:\n",
    "            result = test_file(folder, file_path)\n",
    "            if result:\n",
    "                sequential_aggregation_results[folder] = sequential_aggregation_results.get(folder, []) + [result[0]]\n",
    "\n",
    "    folder_counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-04T21:32:26.890439Z",
     "iopub.status.idle": "2026-02-04T21:32:26.890581Z",
     "shell.execute_reply": "2026-02-04T21:32:26.890518Z",
     "shell.execute_reply.started": "2026-02-04T21:32:26.890511Z"
    }
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for folder in sequential_aggregation_results.keys():\n",
    "    row = {}\n",
    "\n",
    "    results = sequential_aggregation_results[folder]\n",
    "\n",
    "    row[\"folder\"] = folder\n",
    "    row[\"total_files\"] = len(results)\n",
    "    row[\"in_top_1_percentage\"] = round(sum([1 if result == 1 else 0 for result in results]) * 100 / len(results), 2)\n",
    "    row[\"in_top_5_percentage\"] = round(sum([1 if result <= 5 else 0 for result in results]) * 100 / len(results), 2)\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "sequential_aggregation_df = pd.DataFrame(rows)\n",
    "\n",
    "sequential_aggregation_df[\"top_1_product\"] = sequential_aggregation_df[\"total_files\"] * sequential_aggregation_df[\"in_top_1_percentage\"]\n",
    "sequential_aggregation_df[\"top_5_product\"] = sequential_aggregation_df[\"total_files\"] * sequential_aggregation_df[\"in_top_5_percentage\"]\n",
    "\n",
    "print(sum(sequential_aggregation_df[\"top_1_product\"]) / sum(sequential_aggregation_df[\"total_files\"]))\n",
    "print(sum(sequential_aggregation_df[\"top_5_product\"]) / sum(sequential_aggregation_df[\"total_files\"]))\n",
    "\n",
    "sequential_aggregation_df.sort_values(by=\"in_top_5_percentage\", ascending=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mint",
   "language": "python",
   "name": "mint"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
